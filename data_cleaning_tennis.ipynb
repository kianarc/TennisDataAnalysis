{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60698f8f",
   "metadata": {},
   "source": [
    "ایمپورت کتابخانه ها "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128f1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6a15d",
   "metadata": {},
   "source": [
    "تنظیمات "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e23374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ساخت connection string برای SQLAlchemy\n",
    "connection_string = f\"postgresql://developer:m4dtls64soe@87.248.130.241:5885/tennis_db\"\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf98ed4",
   "metadata": {},
   "source": [
    "تست اتصال "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c69c7095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "Tables in database:\n",
      " - matchvenueinfo\n",
      " - matchvotesinfo\n",
      " - oddsinfo\n",
      " - periodinfo\n",
      " - powerinfo\n",
      " - matcheventinfo\n",
      " - gameinfo\n",
      " - matchawayteaminfo\n",
      " - matchhometeaminfo\n",
      " - matchroundinfo\n",
      " - matchseasoninfo\n",
      " - matchtimeinfo\n",
      " - matchtournamentinfo\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn = engine.connect()\n",
    "    print(\"Connection successful!\")\n",
    "    \n",
    "    tables = pd.read_sql(\"SELECT table_name FROM information_schema.tables WHERE table_schema='public'\", conn)\n",
    "    print(\"Tables in database:\")\n",
    "    for table in tables['table_name']:\n",
    "        print(f\" - {table}\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Connection error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d7b02a",
   "metadata": {},
   "source": [
    "خواندن داده ها از جدول "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "307c03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: matchvenueinfo\n",
      "Records: 16749\n",
      "Columns: ['match_id', 'city', 'stadium', 'venue_id', 'country']\n",
      "--------------------------------------------------\n",
      "Reading: matchvotesinfo\n",
      "Records: 16873\n",
      "Columns: ['match_id', 'home_vote', 'away_vote']\n",
      "--------------------------------------------------\n",
      "Reading: oddsinfo\n",
      "Records: 60946\n",
      "Columns: ['match_id', 'market_id', 'market_name', 'is_live', 'suspended', 'initial_fractional_value', 'fractional_value', 'choice_name', 'choice_source_id', 'winnig', 'change']\n",
      "--------------------------------------------------\n",
      "Reading: periodinfo\n",
      "Records: 1358234\n",
      "Columns: ['match_id', 'period', 'statistic_category_name', 'statistic_name', 'home_stat', 'away_stat', 'compare_code', 'statistic_type', 'value_type', 'home_value', 'away_value', 'home_total', 'away_total']\n",
      "--------------------------------------------------\n",
      "Reading: powerinfo\n",
      "Records: 469677\n",
      "Columns: ['match_id', 'set_num', 'game_num', 'value', 'break_occurred']\n",
      "--------------------------------------------------\n",
      "Reading: matcheventinfo\n",
      "Records: 16873\n",
      "Columns: ['match_id', 'first_to_serve', 'home_team_seed', 'away_team_seed', 'custom_id', 'winner_code', 'default_period_count', 'start_datetime', 'match_slug', 'final_result_only']\n",
      "--------------------------------------------------\n",
      "Reading: gameinfo\n",
      "Records: 2549369\n",
      "Columns: ['match_id', 'set_id', 'game_id', 'point_id', 'home_point', 'away_point', 'point_description', 'home_point_type', 'away_point_type', 'home_score', 'away_score', 'serving', 'scoring']\n",
      "--------------------------------------------------\n",
      "Reading: matchawayteaminfo\n",
      "Records: 11690\n",
      "Columns: ['match_id', 'name', 'slug', 'gender', 'user_count', 'residence', 'birthplace', 'height', 'weight', 'plays', 'turned_pro', 'current_prize', 'total_prize', 'player_id', 'current_rank', 'name_code', 'country', 'full_name']\n",
      "--------------------------------------------------\n",
      "Reading: matchhometeaminfo\n",
      "Records: 12389\n",
      "Columns: ['match_id', 'name', 'slug', 'gender', 'user_count', 'residence', 'birthplace', 'height', 'weight', 'plays', 'turned_pro', 'current_prize', 'total_prize', 'player_id', 'current_rank', 'name_code', 'country', 'full_name']\n",
      "--------------------------------------------------\n",
      "Reading: matchroundinfo\n",
      "Records: 9243\n",
      "Columns: ['match_id', 'round_id', 'name', 'slug', 'cup_round_type']\n",
      "--------------------------------------------------\n",
      "Reading: matchseasoninfo\n",
      "Records: 16873\n",
      "Columns: ['match_id', 'season_id', 'name', 'year']\n",
      "--------------------------------------------------\n",
      "Reading: matchtimeinfo\n",
      "Records: 16873\n",
      "Columns: ['match_id', 'period_1', 'period_2', 'period_3', 'period_4', 'period_5', 'current_period_start_timestamp']\n",
      "--------------------------------------------------\n",
      "Reading: matchtournamentinfo\n",
      "Records: 16873\n",
      "Columns: ['match_id', 'tournament_id', 'tournament_name', 'tournament_slug', 'tournament_unique_id', 'tournament_category_name', 'tournament_category_slug', 'user_count', 'ground_type', 'tennis_points', 'has_event_player_statistics', 'crowd_sourcing_enabled', 'has_performance_graph_feature', 'display_inverse_home_away_teams', 'priority', 'competition_type']\n",
      "--------------------------------------------------\n",
      "Data reading completed successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    tables_query = \"SELECT table_name FROM information_schema.tables WHERE table_schema='public'\"\n",
    "    tables = pd.read_sql(tables_query, conn)\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    for table_name in tables['table_name']:\n",
    "        print(f\"Reading: {table_name}\")\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "        all_data[table_name] = df\n",
    "        print(f\"Records: {len(df)}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    raw_data = all_data\n",
    "    print(\"Data reading completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    raw_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81913e",
   "metadata": {},
   "source": [
    "نمایش داده های خام "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adad32b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATA OVERVIEW\n",
      "==================================================\n",
      "\n",
      "Table: matchvenueinfo\n",
      "Shape: (16749, 5)\n",
      "Columns: ['match_id', 'city', 'stadium', 'venue_id', 'country']\n",
      "Data Types:\n",
      "match_id     int64\n",
      "city        object\n",
      "stadium     object\n",
      "venue_id     int64\n",
      "country     object\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id     0\n",
      "city         0\n",
      "stadium      0\n",
      "venue_id     0\n",
      "country     83\n",
      "dtype: int64\n",
      "Duplicate Rows: 0\n",
      "First 2 Rows:\n",
      "   match_id       city        stadium  venue_id      country\n",
      "0  11974053  Groningen  Martini Plaza      7324  Netherlands\n",
      "1  11974066    Vilnius      SEB Arena     36345    Lithuania\n",
      "----------------------------------------\n",
      "\n",
      "Table: matchvotesinfo\n",
      "Shape: (16873, 3)\n",
      "Columns: ['match_id', 'home_vote', 'away_vote']\n",
      "Data Types:\n",
      "match_id     int64\n",
      "home_vote    int64\n",
      "away_vote    int64\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id     0\n",
      "home_vote    0\n",
      "away_vote    0\n",
      "dtype: int64\n",
      "Duplicate Rows: 0\n",
      "First 2 Rows:\n",
      "   match_id  home_vote  away_vote\n",
      "0  11974053        272         61\n",
      "1  11974066        124        579\n",
      "----------------------------------------\n",
      "\n",
      "Table: oddsinfo\n",
      "Shape: (60946, 11)\n",
      "Columns: ['match_id', 'market_id', 'market_name', 'is_live', 'suspended', 'initial_fractional_value', 'fractional_value', 'choice_name', 'choice_source_id', 'winnig', 'change']\n",
      "Data Types:\n",
      "match_id                     int64\n",
      "market_id                    int64\n",
      "market_name                 object\n",
      "is_live                       bool\n",
      "suspended                     bool\n",
      "initial_fractional_value    object\n",
      "fractional_value            object\n",
      "choice_name                 object\n",
      "choice_source_id             int64\n",
      "winnig                      object\n",
      "change                       int64\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id                       0\n",
      "market_id                      0\n",
      "market_name                    0\n",
      "is_live                        0\n",
      "suspended                      0\n",
      "initial_fractional_value       0\n",
      "fractional_value               0\n",
      "choice_name                    0\n",
      "choice_source_id               0\n",
      "winnig                      3900\n",
      "change                         0\n",
      "dtype: int64\n",
      "Duplicate Rows: 26139\n",
      "First 2 Rows:\n",
      "   match_id  market_id market_name  is_live  suspended  \\\n",
      "0  11974053          1   full_time    False      False   \n",
      "1  11974053          1   full_time    False      False   \n",
      "\n",
      "  initial_fractional_value fractional_value choice_name  choice_source_id  \\\n",
      "0                      1/4              1/5           1        1994690951   \n",
      "1                     11/4             10/3           2        1994691017   \n",
      "\n",
      "  winnig  change  \n",
      "0   True      -1  \n",
      "1  False       1  \n",
      "----------------------------------------\n",
      "\n",
      "Table: periodinfo\n",
      "Shape: (1358234, 13)\n",
      "Columns: ['match_id', 'period', 'statistic_category_name', 'statistic_name', 'home_stat', 'away_stat', 'compare_code', 'statistic_type', 'value_type', 'home_value', 'away_value', 'home_total', 'away_total']\n",
      "Data Types:\n",
      "match_id                     int64\n",
      "period                      object\n",
      "statistic_category_name     object\n",
      "statistic_name              object\n",
      "home_stat                   object\n",
      "away_stat                   object\n",
      "compare_code                 int64\n",
      "statistic_type              object\n",
      "value_type                  object\n",
      "home_value                   int64\n",
      "away_value                   int64\n",
      "home_total                 float64\n",
      "away_total                 float64\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id                        0\n",
      "period                          0\n",
      "statistic_category_name         0\n",
      "statistic_name                  0\n",
      "home_stat                       0\n",
      "away_stat                       0\n",
      "compare_code                    0\n",
      "statistic_type                  0\n",
      "value_type                      0\n",
      "home_value                      0\n",
      "away_value                      0\n",
      "home_total                 824739\n",
      "away_total                 824739\n",
      "dtype: int64\n",
      "Duplicate Rows: 611873\n",
      "First 2 Rows:\n",
      "   match_id period statistic_category_name          statistic_name home_stat  \\\n",
      "0  12102573    ALL                  return  break_points_converted         6   \n",
      "1  12102573    ALL           miscellaneous               tiebreaks         0   \n",
      "\n",
      "  away_stat  compare_code statistic_type value_type  home_value  away_value  \\\n",
      "0         1             1       positive      event           6           1   \n",
      "1         0             3       positive      event           0           0   \n",
      "\n",
      "   home_total  away_total  \n",
      "0         NaN         NaN  \n",
      "1         NaN         NaN  \n",
      "----------------------------------------\n",
      "\n",
      "Table: powerinfo\n",
      "Shape: (469677, 5)\n",
      "Columns: ['match_id', 'set_num', 'game_num', 'value', 'break_occurred']\n",
      "Data Types:\n",
      "match_id            int64\n",
      "set_num             int64\n",
      "game_num            int64\n",
      "value             float64\n",
      "break_occurred       bool\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id          0\n",
      "set_num           0\n",
      "game_num          0\n",
      "value             0\n",
      "break_occurred    0\n",
      "dtype: int64\n",
      "Duplicate Rows: 220090\n",
      "First 2 Rows:\n",
      "   match_id  set_num  game_num  value  break_occurred\n",
      "0  11998445        1         1 -52.80           False\n",
      "1  11998445        1         2  48.14           False\n",
      "----------------------------------------\n",
      "\n",
      "Table: matcheventinfo\n",
      "Shape: (16873, 10)\n",
      "Columns: ['match_id', 'first_to_serve', 'home_team_seed', 'away_team_seed', 'custom_id', 'winner_code', 'default_period_count', 'start_datetime', 'match_slug', 'final_result_only']\n",
      "Data Types:\n",
      "match_id                  int64\n",
      "first_to_serve           object\n",
      "home_team_seed           object\n",
      "away_team_seed           object\n",
      "custom_id                object\n",
      "winner_code             float64\n",
      "default_period_count      int64\n",
      "start_datetime            int64\n",
      "match_slug               object\n",
      "final_result_only          bool\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id                    0\n",
      "first_to_serve           6329\n",
      "home_team_seed          12249\n",
      "away_team_seed          12324\n",
      "custom_id                   0\n",
      "winner_code              2380\n",
      "default_period_count        0\n",
      "start_datetime              0\n",
      "match_slug                  0\n",
      "final_result_only           0\n",
      "dtype: int64\n",
      "Duplicate Rows: 0\n",
      "First 2 Rows:\n",
      "   match_id first_to_serve home_team_seed away_team_seed custom_id  \\\n",
      "0  11974053           None           None           None   NNfsQTf   \n",
      "1  11974066           None           None           None   hTfsrpg   \n",
      "\n",
      "   winner_code  default_period_count  start_datetime               match_slug  \\\n",
      "0          1.0                     3      1706878800  switzerland-netherlands   \n",
      "1          2.0                     3      1706871600              ukraine-usa   \n",
      "\n",
      "   final_result_only  \n",
      "0              False  \n",
      "1              False  \n",
      "----------------------------------------\n",
      "\n",
      "Table: gameinfo\n",
      "Shape: (2549369, 13)\n",
      "Columns: ['match_id', 'set_id', 'game_id', 'point_id', 'home_point', 'away_point', 'point_description', 'home_point_type', 'away_point_type', 'home_score', 'away_score', 'serving', 'scoring']\n",
      "Data Types:\n",
      "match_id              int64\n",
      "set_id                int64\n",
      "game_id               int64\n",
      "point_id              int64\n",
      "home_point           object\n",
      "away_point           object\n",
      "point_description     int64\n",
      "home_point_type       int64\n",
      "away_point_type       int64\n",
      "home_score            int64\n",
      "away_score            int64\n",
      "serving               int64\n",
      "scoring               int64\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id             0\n",
      "set_id               0\n",
      "game_id              0\n",
      "point_id             0\n",
      "home_point           0\n",
      "away_point           0\n",
      "point_description    0\n",
      "home_point_type      0\n",
      "away_point_type      0\n",
      "home_score           0\n",
      "away_score           0\n",
      "serving              0\n",
      "scoring              0\n",
      "dtype: int64\n",
      "Duplicate Rows: 1294625\n",
      "First 2 Rows:\n",
      "   match_id  set_id  game_id  point_id home_point away_point  \\\n",
      "0  11999018       1        7         2         40          0   \n",
      "1  11999018       1        6         0         15          0   \n",
      "\n",
      "   point_description  home_point_type  away_point_type  home_score  \\\n",
      "0                  0                1                5           5   \n",
      "1                  0                1                5           4   \n",
      "\n",
      "   away_score  serving  scoring  \n",
      "0           2        1        1  \n",
      "1           2        2        1  \n",
      "----------------------------------------\n",
      "\n",
      "Table: matchawayteaminfo\n",
      "Shape: (11690, 18)\n",
      "Columns: ['match_id', 'name', 'slug', 'gender', 'user_count', 'residence', 'birthplace', 'height', 'weight', 'plays', 'turned_pro', 'current_prize', 'total_prize', 'player_id', 'current_rank', 'name_code', 'country', 'full_name']\n",
      "Data Types:\n",
      "match_id           int64\n",
      "name              object\n",
      "slug              object\n",
      "gender            object\n",
      "user_count         int64\n",
      "residence         object\n",
      "birthplace        object\n",
      "height           float64\n",
      "weight           float64\n",
      "plays             object\n",
      "turned_pro        object\n",
      "current_prize    float64\n",
      "total_prize      float64\n",
      "player_id          int64\n",
      "current_rank     float64\n",
      "name_code         object\n",
      "country           object\n",
      "full_name         object\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id            0\n",
      "name                0\n",
      "slug                0\n",
      "gender             16\n",
      "user_count          0\n",
      "residence        8297\n",
      "birthplace       4995\n",
      "height           5165\n",
      "weight           8350\n",
      "plays            5921\n",
      "turned_pro       9395\n",
      "current_prize     107\n",
      "total_prize        63\n",
      "player_id           0\n",
      "current_rank      155\n",
      "name_code           0\n",
      "country             2\n",
      "full_name           0\n",
      "dtype: int64\n",
      "Duplicate Rows: 0\n",
      "First 2 Rows:\n",
      "   match_id                name                   slug gender  user_count  \\\n",
      "0  11998445  Auger-Aliassime F.  auger-aliassime-felix      M       23318   \n",
      "1  11998446          Cobolli F.         flavio-cobolli      M        5995   \n",
      "\n",
      "             residence        birthplace  height  weight         plays  \\\n",
      "0  Monte Carlo, Monaco  Montreal, Canada    1.93    87.0  right-handed   \n",
      "1                 Rome   Florence, Italy    1.83    71.0  right-handed   \n",
      "\n",
      "  turned_pro  current_prize  total_prize  player_id  current_rank name_code  \\\n",
      "0       2017       218538.0   10166964.0     192013          30.0       AUG   \n",
      "1       None       178358.0     808536.0     273680          69.0       COB   \n",
      "\n",
      "  country               full_name  \n",
      "0  Canada  Auger-Aliassime, Felix  \n",
      "1   Italy          Flavio Cobolli  \n",
      "----------------------------------------\n",
      "\n",
      "Table: matchhometeaminfo\n",
      "Shape: (12389, 18)\n",
      "Columns: ['match_id', 'name', 'slug', 'gender', 'user_count', 'residence', 'birthplace', 'height', 'weight', 'plays', 'turned_pro', 'current_prize', 'total_prize', 'player_id', 'current_rank', 'name_code', 'country', 'full_name']\n",
      "Data Types:\n",
      "match_id           int64\n",
      "name              object\n",
      "slug              object\n",
      "gender            object\n",
      "user_count         int64\n",
      "residence         object\n",
      "birthplace        object\n",
      "height           float64\n",
      "weight           float64\n",
      "plays             object\n",
      "turned_pro        object\n",
      "current_prize    float64\n",
      "total_prize      float64\n",
      "player_id          int64\n",
      "current_rank     float64\n",
      "name_code         object\n",
      "country           object\n",
      "full_name         object\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id             0\n",
      "name                 0\n",
      "slug                 0\n",
      "gender              19\n",
      "user_count           0\n",
      "residence         8914\n",
      "birthplace        5283\n",
      "height            5504\n",
      "weight            8996\n",
      "plays             6363\n",
      "turned_pro       10057\n",
      "current_prize       63\n",
      "total_prize         34\n",
      "player_id            0\n",
      "current_rank       136\n",
      "name_code            0\n",
      "country              4\n",
      "full_name            0\n",
      "dtype: int64\n",
      "Duplicate Rows: 0\n",
      "First 2 Rows:\n",
      "   match_id          name                slug gender  user_count  \\\n",
      "0  11998445     Cazaux A.       cazaux-arthur      M        4481   \n",
      "1  11998446  Lestienne C.  lestienne-constant      M        2032   \n",
      "\n",
      "       residence           birthplace  height  weight         plays  \\\n",
      "0           None  Montpellier, France    1.83     NaN  right-handed   \n",
      "1  Paris, France       Amiens, France    1.80    72.0  right-handed   \n",
      "\n",
      "  turned_pro  current_prize  total_prize  player_id  current_rank name_code  \\\n",
      "0       None       233290.0     643582.0     287803          86.0       CAZ   \n",
      "1       2012       105335.0    1404567.0      62790          99.0       LES   \n",
      "\n",
      "  country            full_name  \n",
      "0  France       Cazaux, Arthur  \n",
      "1  France  Lestienne, Constant  \n",
      "----------------------------------------\n",
      "\n",
      "Table: matchroundinfo\n",
      "Shape: (9243, 5)\n",
      "Columns: ['match_id', 'round_id', 'name', 'slug', 'cup_round_type']\n",
      "Data Types:\n",
      "match_id            int64\n",
      "round_id            int64\n",
      "name               object\n",
      "slug               object\n",
      "cup_round_type    float64\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id             0\n",
      "round_id             0\n",
      "name                 0\n",
      "slug                 0\n",
      "cup_round_type    1396\n",
      "dtype: int64\n",
      "Duplicate Rows: 0\n",
      "First 2 Rows:\n",
      "   match_id  round_id         name         slug  cup_round_type\n",
      "0  11998445         5  Round of 16  round-of-16             8.0\n",
      "1  11998446         5  Round of 16  round-of-16             8.0\n",
      "----------------------------------------\n",
      "\n",
      "Table: matchseasoninfo\n",
      "Shape: (16873, 4)\n",
      "Columns: ['match_id', 'season_id', 'name', 'year']\n",
      "Data Types:\n",
      "match_id      int64\n",
      "season_id     int64\n",
      "name         object\n",
      "year          int64\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id     0\n",
      "season_id    0\n",
      "name         0\n",
      "year         0\n",
      "dtype: int64\n",
      "Duplicate Rows: 0\n",
      "First 2 Rows:\n",
      "   match_id  season_id            name  year\n",
      "0  11974053      55584  Davis Cup 2024  2024\n",
      "1  11974066      55584  Davis Cup 2024  2024\n",
      "----------------------------------------\n",
      "\n",
      "Table: matchtimeinfo\n",
      "Shape: (16873, 7)\n",
      "Columns: ['match_id', 'period_1', 'period_2', 'period_3', 'period_4', 'period_5', 'current_period_start_timestamp']\n",
      "Data Types:\n",
      "match_id                           int64\n",
      "period_1                          object\n",
      "period_2                          object\n",
      "period_3                          object\n",
      "period_4                          object\n",
      "period_5                          object\n",
      "current_period_start_timestamp    object\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id                              0\n",
      "period_1                           6546\n",
      "period_2                           6630\n",
      "period_3                          13740\n",
      "period_4                          16873\n",
      "period_5                          16873\n",
      "current_period_start_timestamp     6243\n",
      "dtype: int64\n",
      "Duplicate Rows: 0\n",
      "First 2 Rows:\n",
      "   match_id period_1 period_2 period_3 period_4 period_5  \\\n",
      "0  11974053     None     None     None     None     None   \n",
      "1  11974066     None     None     None     None     None   \n",
      "\n",
      "  current_period_start_timestamp  \n",
      "0                           None  \n",
      "1                           None  \n",
      "----------------------------------------\n",
      "\n",
      "Table: matchtournamentinfo\n",
      "Shape: (16873, 16)\n",
      "Columns: ['match_id', 'tournament_id', 'tournament_name', 'tournament_slug', 'tournament_unique_id', 'tournament_category_name', 'tournament_category_slug', 'user_count', 'ground_type', 'tennis_points', 'has_event_player_statistics', 'crowd_sourcing_enabled', 'has_performance_graph_feature', 'display_inverse_home_away_teams', 'priority', 'competition_type']\n",
      "Data Types:\n",
      "match_id                             int64\n",
      "tournament_id                        int64\n",
      "tournament_name                     object\n",
      "tournament_slug                     object\n",
      "tournament_unique_id                object\n",
      "tournament_category_name            object\n",
      "tournament_category_slug            object\n",
      "user_count                           int64\n",
      "ground_type                         object\n",
      "tennis_points                       object\n",
      "has_event_player_statistics           bool\n",
      "crowd_sourcing_enabled                bool\n",
      "has_performance_graph_feature         bool\n",
      "display_inverse_home_away_teams       bool\n",
      "priority                             int64\n",
      "competition_type                   float64\n",
      "dtype: object\n",
      "Missing Values:\n",
      "match_id                               0\n",
      "tournament_id                          0\n",
      "tournament_name                        0\n",
      "tournament_slug                        0\n",
      "tournament_unique_id               16873\n",
      "tournament_category_name               0\n",
      "tournament_category_slug               0\n",
      "user_count                             0\n",
      "ground_type                          273\n",
      "tennis_points                      14930\n",
      "has_event_player_statistics            0\n",
      "crowd_sourcing_enabled                 0\n",
      "has_performance_graph_feature          0\n",
      "display_inverse_home_away_teams        0\n",
      "priority                               0\n",
      "competition_type                     224\n",
      "dtype: int64\n",
      "Duplicate Rows: 0\n",
      "First 2 Rows:\n",
      "   match_id  tournament_id tournament_name tournament_slug  \\\n",
      "0  11974053          70826      Qualifiers      qualifiers   \n",
      "1  11974066          70826      Qualifiers      qualifiers   \n",
      "\n",
      "  tournament_unique_id tournament_category_name tournament_category_slug  \\\n",
      "0                 None                Davis Cup                davis-cup   \n",
      "1                 None                Davis Cup                davis-cup   \n",
      "\n",
      "   user_count ground_type tennis_points  has_event_player_statistics  \\\n",
      "0        6909        None          None                        False   \n",
      "1        6909        None          None                        False   \n",
      "\n",
      "   crowd_sourcing_enabled  has_performance_graph_feature  \\\n",
      "0                   False                          False   \n",
      "1                   False                          False   \n",
      "\n",
      "   display_inverse_home_away_teams  priority  competition_type  \n",
      "0                            False         0               2.0  \n",
      "1                            False         0               2.0  \n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"RAW DATA OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if raw_data is None:\n",
    "    print(\"No data available\")\n",
    "else:\n",
    "    for table_name, df in raw_data.items():\n",
    "        print(f\"\\nTable: {table_name}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(f\"Data Types:\")\n",
    "        print(df.dtypes)\n",
    "        print(f\"Missing Values:\")\n",
    "        print(df.isnull().sum())\n",
    "        print(f\"Duplicate Rows: {df.duplicated().sum()}\")\n",
    "        print(f\"First 2 Rows:\")\n",
    "        print(df.head(2))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63aafc3",
   "metadata": {},
   "source": [
    "تمیز کردن داده "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "519cbe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Cleaning: matchvenueinfo\n",
      "Original: 16749 rows, 5 columns\n",
      "Final: 16749 rows, 5 columns\n",
      "Duplicates removed: 0\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: matchvotesinfo\n",
      "Original: 16873 rows, 3 columns\n",
      "Final: 16873 rows, 3 columns\n",
      "Duplicates removed: 0\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: oddsinfo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 60946 rows, 11 columns\n",
      "Final: 34807 rows, 11 columns\n",
      "Duplicates removed: 26139\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: periodinfo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 1358234 rows, 13 columns\n",
      "Final: 746361 rows, 13 columns\n",
      "Duplicates removed: 611873\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: powerinfo\n",
      "Original: 469677 rows, 5 columns\n",
      "Final: 249587 rows, 5 columns\n",
      "Duplicates removed: 220090\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: matcheventinfo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 16873 rows, 10 columns\n",
      "Final: 16873 rows, 10 columns\n",
      "Duplicates removed: 0\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: gameinfo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 2549369 rows, 13 columns\n",
      "Final: 1254744 rows, 13 columns\n",
      "Duplicates removed: 1294625\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: matchawayteaminfo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 11690 rows, 18 columns\n",
      "Final: 11690 rows, 18 columns\n",
      "Duplicates removed: 0\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: matchhometeaminfo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 12389 rows, 18 columns\n",
      "Final: 12389 rows, 18 columns\n",
      "Duplicates removed: 0\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: matchroundinfo\n",
      "Original: 9243 rows, 5 columns\n",
      "Final: 9243 rows, 5 columns\n",
      "Duplicates removed: 0\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: matchseasoninfo\n",
      "Original: 16873 rows, 4 columns\n",
      "Final: 16873 rows, 4 columns\n",
      "Duplicates removed: 0\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: matchtimeinfo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 16873 rows, 7 columns\n",
      "Final: 16873 rows, 7 columns\n",
      "Duplicates removed: 0\n",
      "Remaining missing values: 0\n",
      "\n",
      "==================================================\n",
      "Cleaning: matchtournamentinfo\n",
      "Original: 16873 rows, 16 columns\n",
      "Final: 16873 rows, 16 columns\n",
      "Duplicates removed: 0\n",
      "Remaining missing values: 0\n",
      "\n",
      "Data cleaning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
      "C:\\Users\\SETIYA\\AppData\\Local\\Temp\\ipykernel_16100\\4117781728.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n"
     ]
    }
   ],
   "source": [
    "def clean_dataframe(df, table_name):\n",
    "    print(f\"Cleaning: {table_name}\")\n",
    "    \n",
    "    clean_df = df.copy()\n",
    "    original_shape = clean_df.shape\n",
    "    \n",
    "    # حذف رکوردهای کاملاً تکراری\n",
    "    initial_rows = len(clean_df)\n",
    "    clean_df = clean_df.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(clean_df)\n",
    "    \n",
    "    \n",
    "    for col in clean_df.columns:\n",
    "        missing_count = clean_df[col].isnull().sum()\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            \n",
    "            if clean_df[col].dtype in ['int64', 'float64']:\n",
    "                clean_df[col] = clean_df[col].fillna(clean_df[col].median())\n",
    "            \n",
    "            elif clean_df[col].dtype == 'object':\n",
    "                clean_df[col] = clean_df[col].fillna('Unknown')\n",
    "            \n",
    "            elif 'date' in col.lower() or 'time' in col.lower():\n",
    "                clean_df[col] = clean_df[col].fillna(pd.NaT)\n",
    "    \n",
    "    # تبدیل نوع داده‌ها به فرمت مناسب\n",
    "    for col in clean_df.columns:\n",
    "        if clean_df[col].dtype == 'object':\n",
    "            try:\n",
    "                \n",
    "                clean_df[col] = pd.to_datetime(clean_df[col], errors='ignore')\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # استانداردسازی \n",
    "    text_columns = clean_df.select_dtypes(include=['object']).columns\n",
    "    for col in text_columns:\n",
    "       \n",
    "        clean_df[col] = clean_df[col].astype(str)\n",
    "        clean_df[col] = clean_df[col].str.strip()\n",
    "      \n",
    "        clean_df[col] = clean_df[col].str.lower()\n",
    "      \n",
    "        clean_df[col] = clean_df[col].replace({'nan': 'unknown', 'none': 'unknown', 'null': 'unknown'})\n",
    "    \n",
    "    \n",
    "    print(f\"Original: {original_shape[0]} rows, {original_shape[1]} columns\")\n",
    "    print(f\"Final: {len(clean_df)} rows, {len(clean_df.columns)} columns\")\n",
    "    print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "    print(f\"Remaining missing values: {clean_df.isnull().sum().sum()}\")\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "\n",
    "cleaned_data = {}\n",
    "\n",
    "if raw_data is not None:\n",
    "    for table_name, df in raw_data.items():\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        cleaned_df = clean_dataframe(df, table_name)\n",
    "        cleaned_data[table_name] = cleaned_df\n",
    "    print(\"\\nData cleaning completed!\")\n",
    "else:\n",
    "    print(\"No data to clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bd0cd",
   "metadata": {},
   "source": [
    "نمایش داده های تمیز"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8188939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANED DATA SUMMARY\n",
      "==================================================\n",
      "Total tables processed: 13\n",
      "\n",
      "Table: matchvenueinfo\n",
      "  Rows: 16,749\n",
      "  Columns: 5\n",
      "  Memory usage: 3.0 MB\n",
      "  Numeric columns: 2\n",
      "  Text columns: 3\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'city', 'stadium']...\n",
      "----------------------------------------\n",
      "Table: matchvotesinfo\n",
      "  Rows: 16,873\n",
      "  Columns: 3\n",
      "  Memory usage: 0.4 MB\n",
      "  Numeric columns: 3\n",
      "  Text columns: 0\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'home_vote', 'away_vote']...\n",
      "----------------------------------------\n",
      "Table: oddsinfo\n",
      "  Rows: 34,807\n",
      "  Columns: 11\n",
      "  Memory usage: 10.3 MB\n",
      "  Numeric columns: 4\n",
      "  Text columns: 5\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'market_id', 'market_name']...\n",
      "----------------------------------------\n",
      "Table: periodinfo\n",
      "  Rows: 746,361\n",
      "  Columns: 13\n",
      "  Memory usage: 318.7 MB\n",
      "  Numeric columns: 6\n",
      "  Text columns: 7\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'period', 'statistic_category_name']...\n",
      "----------------------------------------\n",
      "Table: powerinfo\n",
      "  Rows: 249,587\n",
      "  Columns: 5\n",
      "  Memory usage: 9.8 MB\n",
      "  Numeric columns: 4\n",
      "  Text columns: 0\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'set_num', 'game_num']...\n",
      "----------------------------------------\n",
      "Table: matcheventinfo\n",
      "  Rows: 16,873\n",
      "  Columns: 10\n",
      "  Memory usage: 5.1 MB\n",
      "  Numeric columns: 4\n",
      "  Text columns: 5\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'first_to_serve', 'home_team_seed']...\n",
      "----------------------------------------\n",
      "Table: gameinfo\n",
      "  Rows: 1,254,744\n",
      "  Columns: 13\n",
      "  Memory usage: 236.4 MB\n",
      "  Numeric columns: 11\n",
      "  Text columns: 2\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'set_id', 'game_id']...\n",
      "----------------------------------------\n",
      "Table: matchawayteaminfo\n",
      "  Rows: 11,690\n",
      "  Columns: 18\n",
      "  Memory usage: 7.2 MB\n",
      "  Numeric columns: 8\n",
      "  Text columns: 10\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'name', 'slug']...\n",
      "----------------------------------------\n",
      "Table: matchhometeaminfo\n",
      "  Rows: 12,389\n",
      "  Columns: 18\n",
      "  Memory usage: 7.6 MB\n",
      "  Numeric columns: 8\n",
      "  Text columns: 10\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'name', 'slug']...\n",
      "----------------------------------------\n",
      "Table: matchroundinfo\n",
      "  Rows: 9,243\n",
      "  Columns: 5\n",
      "  Memory usage: 1.3 MB\n",
      "  Numeric columns: 3\n",
      "  Text columns: 2\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'round_id', 'name']...\n",
      "----------------------------------------\n",
      "Table: matchseasoninfo\n",
      "  Rows: 16,873\n",
      "  Columns: 4\n",
      "  Memory usage: 1.7 MB\n",
      "  Numeric columns: 3\n",
      "  Text columns: 1\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'season_id', 'name']...\n",
      "----------------------------------------\n",
      "Table: matchtimeinfo\n",
      "  Rows: 16,873\n",
      "  Columns: 7\n",
      "  Memory usage: 5.5 MB\n",
      "  Numeric columns: 1\n",
      "  Text columns: 6\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'period_1', 'period_2']...\n",
      "----------------------------------------\n",
      "Table: matchtournamentinfo\n",
      "  Rows: 16,873\n",
      "  Columns: 16\n",
      "  Memory usage: 8.1 MB\n",
      "  Numeric columns: 5\n",
      "  Text columns: 7\n",
      "  Date columns: 0\n",
      "  Missing values: 0\n",
      "  Duplicate rows: 0\n",
      "  Sample columns: ['match_id', 'tournament_id', 'tournament_name']...\n",
      "----------------------------------------\n",
      "\n",
      "TOTALS:\n",
      "All tables - Rows: 2,419,935, Columns: 128\n",
      "\n",
      "Next: Run cell 8 to save as Parquet files\n"
     ]
    }
   ],
   "source": [
    "print(\"CLEANED DATA SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if cleaned_data:\n",
    "    total_tables = len(cleaned_data)\n",
    "    total_rows = 0\n",
    "    total_columns = 0\n",
    "    \n",
    "    print(f\"Total tables processed: {total_tables}\\n\")\n",
    "    \n",
    "    for table_name, df in cleaned_data.items():\n",
    "        print(f\"Table: {table_name}\")\n",
    "        print(f\"  Rows: {len(df):,}\")\n",
    "        print(f\"  Columns: {len(df.columns)}\")\n",
    "        print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "        \n",
    "        # اطلاعات ستون‌ها\n",
    "        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        text_cols = df.select_dtypes(include=['object']).columns\n",
    "        date_cols = df.select_dtypes(include=['datetime64']).columns\n",
    "        \n",
    "        print(f\"  Numeric columns: {len(numeric_cols)}\")\n",
    "        print(f\"  Text columns: {len(text_cols)}\")\n",
    "        print(f\"  Date columns: {len(date_cols)}\")\n",
    "        \n",
    "        # وضعیت داده‌های خالی\n",
    "        missing_total = df.isnull().sum().sum()\n",
    "        if missing_total > 0:\n",
    "            print(f\"  Missing values: {missing_total}\")\n",
    "        else:\n",
    "            print(f\"  Missing values: 0\")\n",
    "        \n",
    "        # وضعیت تکراری‌ها\n",
    "        duplicates = df.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            print(f\"  Duplicate rows: {duplicates}\")\n",
    "        else:\n",
    "            print(f\"  Duplicate rows: 0\")\n",
    "        \n",
    "        # نمایش نمونه‌ای از داده‌ها\n",
    "        print(f\"  Sample columns: {list(df.columns[:3])}...\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "        total_rows += len(df)\n",
    "        total_columns += len(df.columns)\n",
    "    \n",
    "    print(f\"\\nTOTALS:\")\n",
    "    print(f\"All tables - Rows: {total_rows:,}, Columns: {total_columns}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No cleaned data available\")\n",
    "\n",
    "print(\"\\nNext: Run cell 8 to save as Parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8816a81",
   "metadata": {},
   "source": [
    "ذخیره سازی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b590ed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: matchvenueinfo\n",
      "Saved: cleaned_data_pickle\\cleaned_matchvenueinfo.pkl (0.8 MB)\n",
      "Rows: 16,749, Columns: 5\n",
      "----------------------------------------\n",
      "Saving: matchvotesinfo\n",
      "Saved: cleaned_data_pickle\\cleaned_matchvotesinfo.pkl (0.4 MB)\n",
      "Rows: 16,873, Columns: 3\n",
      "----------------------------------------\n",
      "Saving: oddsinfo\n",
      "Saved: cleaned_data_pickle\\cleaned_oddsinfo.pkl (2.7 MB)\n",
      "Rows: 34,807, Columns: 11\n",
      "----------------------------------------\n",
      "Saving: periodinfo\n",
      "Saved: cleaned_data_pickle\\cleaned_periodinfo.pkl (89.5 MB)\n",
      "Rows: 746,361, Columns: 13\n",
      "----------------------------------------\n",
      "Saving: powerinfo\n",
      "Saved: cleaned_data_pickle\\cleaned_powerinfo.pkl (9.8 MB)\n",
      "Rows: 249,587, Columns: 5\n",
      "----------------------------------------\n",
      "Saving: matcheventinfo\n",
      "Saved: cleaned_data_pickle\\cleaned_matcheventinfo.pkl (1.4 MB)\n",
      "Rows: 16,873, Columns: 10\n",
      "----------------------------------------\n",
      "Saving: gameinfo\n",
      "Saved: cleaned_data_pickle\\cleaned_gameinfo.pkl (126.3 MB)\n",
      "Rows: 1,254,744, Columns: 13\n",
      "----------------------------------------\n",
      "Saving: matchawayteaminfo\n",
      "Saved: cleaned_data_pickle\\cleaned_matchawayteaminfo.pkl (2.0 MB)\n",
      "Rows: 11,690, Columns: 18\n",
      "----------------------------------------\n",
      "Saving: matchhometeaminfo\n",
      "Saved: cleaned_data_pickle\\cleaned_matchhometeaminfo.pkl (2.2 MB)\n",
      "Rows: 12,389, Columns: 18\n",
      "----------------------------------------\n",
      "Saving: matchroundinfo\n",
      "Saved: cleaned_data_pickle\\cleaned_matchroundinfo.pkl (0.5 MB)\n",
      "Rows: 9,243, Columns: 5\n",
      "----------------------------------------\n",
      "Saving: matchseasoninfo\n",
      "Saved: cleaned_data_pickle\\cleaned_matchseasoninfo.pkl (1.0 MB)\n",
      "Rows: 16,873, Columns: 4\n",
      "----------------------------------------\n",
      "Saving: matchtimeinfo\n",
      "Saved: cleaned_data_pickle\\cleaned_matchtimeinfo.pkl (1.1 MB)\n",
      "Rows: 16,873, Columns: 7\n",
      "----------------------------------------\n",
      "Saving: matchtournamentinfo\n",
      "Saved: cleaned_data_pickle\\cleaned_matchtournamentinfo.pkl (2.9 MB)\n",
      "Rows: 16,873, Columns: 16\n",
      "----------------------------------------\n",
      "All files saved as Pickle successfully!\n"
     ]
    }
   ],
   "source": [
    "def save_as_pickle(cleaned_data):\n",
    "    output_dir = \"cleaned_data_pickle\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for table_name, df in cleaned_data.items():\n",
    "        print(f\"Saving: {table_name}\")\n",
    "        \n",
    "       \n",
    "        pickle_path = os.path.join(output_dir, f\"cleaned_{table_name}.pkl\")\n",
    "        \n",
    "       \n",
    "        df.to_pickle(pickle_path)\n",
    "        \n",
    "        \n",
    "        file_size = os.path.getsize(pickle_path) / (1024 * 1024)\n",
    "        print(f\"Saved: {pickle_path} ({file_size:.1f} MB)\")\n",
    "        print(f\"Rows: {len(df):,}, Columns: {len(df.columns)}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "if cleaned_data:\n",
    "    save_as_pickle(cleaned_data)\n",
    "    print(\"All files saved as Pickle successfully!\")\n",
    "else:\n",
    "    print(\"No data to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tennis)",
   "language": "python",
   "name": "tennis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
